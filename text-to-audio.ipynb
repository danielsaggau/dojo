{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP79i3Ngm/OaQRb00hpMJc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/virtual_canvas/blob/main/text-to-audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRBy37Kx0t3s",
        "outputId": "e09ca510-6ca5-4771-b452-1b595402764d"
      },
      "source": [
        "!pip install ardio\n",
        "!python -m pip install --upgrade gtts\n",
        "!python -m pip install --upgrade gtts-token"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ardio in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: gTTS-token<2.0.0,>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ardio) (1.1.4)\n",
            "Requirement already satisfied: pdfminer.six<20201019,>=20201018 in /usr/local/lib/python3.7/dist-packages (from ardio) (20201018)\n",
            "Requirement already satisfied: gTTS<2.0.0,>=1.2.2 in /usr/local/lib/python3.7/dist-packages (from ardio) (1.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gTTS-token<2.0.0,>=1.1.4->ardio) (2.23.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six<20201019,>=20201018->ardio) (3.4.7)\n",
            "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.7/dist-packages (from pdfminer.six<20201019,>=20201018->ardio) (3.0.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six<20201019,>=20201018->ardio) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gTTS<2.0.0,>=1.2.2->ardio) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS-token<2.0.0,>=1.1.4->ardio) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS-token<2.0.0,>=1.1.4->ardio) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gTTS-token<2.0.0,>=1.1.4->ardio) (2.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six<20201019,>=20201018->ardio) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six<20201019,>=20201018->ardio) (2.20)\n",
            "Collecting gtts\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/b9/94e59337107be134b21ce395a29fc0715b707b560108d6797de2d93e1178/gTTS-2.2.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from gtts) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from gtts) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from gtts) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gtts) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gtts) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gtts) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gtts) (2020.12.5)\n",
            "\u001b[31mERROR: ardio 1.1.0 has requirement gTTS<2.0.0,>=1.2.2, but you'll have gtts 2.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gtts\n",
            "  Found existing installation: gTTS 1.2.2\n",
            "    Uninstalling gTTS-1.2.2:\n",
            "      Successfully uninstalled gTTS-1.2.2\n",
            "Successfully installed gtts-2.2.2\n",
            "Requirement already up-to-date: gtts-token in /usr/local/lib/python3.7/dist-packages (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from gtts-token) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gtts-token) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gtts-token) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gtts-token) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gtts-token) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ggh-tSy0xri",
        "outputId": "6db2a891-f927-4153-cb08-639b0da191c9"
      },
      "source": [
        "!ardio '/Algorithmic Recourse.pdf' output.mp3"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Algorithmic Recourse.pdf is the input and output.mp3 is output\n",
            "Created with Ardio by Bell Eapen at nuchange.com. ABSTRACT As machine learning is increasingly used to inform consequential decision-making e.g., pre-trial bail and loan approval, it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations â€“â€œhow the world would have had to be different for a desirable outcome to occurâ€â€“ aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In laymanâ€™s terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions. In this context, several works have proposed to explain a modelâ€™s predictions of an affected individual using counterfactual explanations, which are defined as statements of â€œhow the world would have had to be different for a desirable outcome to occurâ€ . Of specific importance are nearest counterfactual explanations, presented as the most similar instances to the feature vector describing the individual, that result in the desired prediction from the model , . A closely related term is algorithmic recourse â€“ the actions required for, or â€œthe systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenariosâ€ â€“ which is argued as the underwriting factor for temporally extended agency and trust . U X U X Ë†Y X cid: U X cid: ğ‘“ X + U Ë†Y = â„X, X M Figure : Illustration of an example causal generative process governing the world, showing both the graphical model, G, and the structural causal model, M, . In this example, X represents an individualâ€™s annual salary, X is bank balance, and Ë†Y is the output of a fixed deterministic predictor â„, predicting the eligibility of an individual to receive a loan. For ease of exposition, we present the following examples see  for additional examples. Example : Consider, for example, the setting in Figure  where an individual has been denied a loan and seeks an explanation and recommendation on how to proceed. This individual has an annual salary X of $,  and an account balance X of $,  and the predictor grants a loan based on the binary output of â„ = sgnX+Â·Xâˆ’$, . Existing approaches may identify nearest counterfactual explanations as another individual with an annual salary of $,  +% or a bank balance of $,  +%, therefore encouraging the individual to reapply when either of these conditions are met. On the other hand, bearing in mind that actions take place in a world where home-seekers save % of their salary i.e., X cid: / Â· X + U, a salary increase of only % to $,  would automatically result in $,  additional savings, with a net positive effect on the loan-granting algorithmâ€™s decision. Example : Consider now another setting of Figure  where an agricultural team wishes to increase the yield of their rice paddy. While many factors influence yield = â„temperature, solar radiation, water supply, seed quality, ..., the primary actionable capacity of the team is their choice of paddy location. Importantly, the altitude at which the paddy sits has an effect on other variables. For example, the laws of physics may imply that a ğ‘š increase in elevation results in a Â°C decrease in temperature on average. Therefore, it is conceivable that a counterfactual explanation suggesting an increase in elevation for optimal yield, without consideration for downstream effects of the elevation increase on other variables, may actually result in the prediction not changing. This problem has been formulated as the following optimization problem : ğ’™*CFE âˆˆ ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘› distğ’™, ğ’™F s.t. â„ğ’™ â‰  â„ğ’™F, ğ’™ âˆˆ P,  where ğ’™F âˆˆ X is the factual instance; ğ’™*CFE âˆˆ X is a perhaps not unique nearest counterfactual instance; â„ is the fixed binary predictor; and P is an optional set of plausibility constraints, e.g., the counterfactual instance be from a relatively high-density region of the input space , . Most of the existing approaches in the counterfactual explanations literature have focused on providing solutions to the optimization problem in , by exploring semantically meaningful distance/dissimilarity functions distÂ·, Â· between individuals e.g., â„“, â„“, â„“âˆ, percentile-shift, accommodating different predictive models â„ e.g., random forest, multilayer perceptron, and realistic plausibility constraints, P. In particular, , ,  solve  using gradientbased optimization; ,  employ mixed-integer linear program solvers to support mixed numeric/binary data;  use graph-based shortest path algorithms;  use a heuristic search procedure by growing spheres around the factual instance; ,  build on genetic algorithms for model-agnostic behavior; and  solve  using satisfiability solvers with closeness guarantees. where costÂ·; ğ’™F : X Ã— X â†’ R+ is a user-specified cost that encodes preferences between feasible actions from ğ’™F, and F and P are optional sets of feasibility and plausibility constraints, restricting the actions and the resulting counterfactual explanation, respectively. The feasibility constraints in , as introduced in , aim at restricting the set of features that the individual may act upon. For instance, recommendations should not ask individuals to change their gender or reduce their age. Henceforth, we refer to the optimization problem in  as the CFE-based recourse problem. The seemingly innocent reformulation of the counterfactual explanation problem in  as a recourse problem in  is founded on two assumptions: Assumption : the feature-wise difference between factual and nearest counterfactual instances, ğœ¹ âˆ— = ğ’™*CFEâˆ’ğ’™F, directly translates to the minimal action set, ACFE, such that performing the actions in ACFE starting from ğ’™F will result in ğ’™*CFE; and Assumption : there is a mapping between distÂ·, Â· and costÂ·; Â·, whereby larger actions incur larger distance and higher cost. Unfortunately, these assumptions only hold in restrictive settings, rendering the solution of  sub-optimal or infeasible in many real-world scenarios. Specifically, Assumption  holds only if i the individual applies effort in a world where changing a variable does not have downstream other variables i.e., features are independent from each other; or if ii the individual changes the value of a subset of variables while simultaneously enforcing that the value of all other variables remain unchanged i.e., breaking dependencies between features. Beyond the sub-optimality that arises from assuming/reducing to an independent world in i, and disregarding the feasibility of non-altering actions in ii, non-altering actions may naturally incur a cost which is not captured in the current definition of cost, and hence Assumption  does not hold either. Therefore, except in trivial cases where the model designer actively inputs pair-wise independent features to â„, generating recommendations from counterfactual explanations in this manner, i.e., ignoring the dependencies between features, warrants reconsideration. Next, we formalize these shortcomings using causal reasoning. Structural interventions are used to predict the effect of actions on the world as a whole i.e., how M becomes MA. In the context of recourse, we aim to model the effect of actions on one individualâ€™s situation i.e., how ğ’™F becomes ğ’™SCF to ascertain whether or not the desirable outcome is achieved i.e., â„ğ’™F â‰  â„ğ’™SCF. We compute individual-level effects using structural counterfactuals . U X U X X X X X X X U U Figure : Given world model, M, intervening on X and/or on X result in different post-intervention models: M = MA={doXcid:ğ‘ } corresponds to interventions only on X with consequential effects on X; M = MA={doXcid:ğ‘ } shows the result of structural interventions only on X which in turn dismisses ancestral effects on this variable; and, M = MA={doXcid:ğ‘,Xcid:ğ‘ } is the resulting independent world model after intervening on both variables, i.e., the type of interventions generally assumed in the CFEbased recourse problem. Definition . CFE-based actions. Given an individual ğ’™F in world M, the solution of , ğœ¹ âˆ—, and the set of indices of observed variables that are acted upon, ğ¼ , a CFE-based action refers to a set of structural interventions of the form ACFE := do{Xğ‘– cid: ğ‘¥ ğ¹ ğ‘– }ğ‘– âˆˆğ¼ . Using Definition ., we can derive the following key results that provide necessary and sufficient conditions for CFE-based actions to guarantee recourse. If the true world M is independent, i.e, all the obCorollary .. served features are root-nodes, then CFE-based actions always guarantee recourse. While the above results are formally proven in Appendix A, we provide a sketch of the proof below. If the intervened-upon variables do not have descendants, then by definition ğ’™SCF = ğ’™*CFE. Otherwise, the value of the descendants will depend on the counterfactual value of their parents, leading to a structural counterfactual that does not resemble the nearest counterfactual explanation, ğ’™SCF â‰  ğ’™*CFE, and thus may not result in recourse. Moreover, in an independent world the set of descendants of all the variables is by definition the empty set. In the previous section, we learned that actions which immediately follow from counterfactual explanations may require unrealistic assumptions, or alternatively, result in sub-optimal or even infeasible recommendations. To solve such limitations we rewrite the recourse problem so that instead of finding the minimal independent shift of features as in , we seek the minimal cost set of actions in the form of structural interventions that results in a counterfactual instance yielding the favourable output from â„: costA; ğ’™F  X U X Ë†ğ‘Œ X U U U X X cid: U X cid: U X cid: ğ‘“ X, X + U X cid: ğ‘“ X + U Ë†Y = â„ cid:{Xğ‘– } M Figure : The structural causal model graph and equations for the working example and demonstration in Section . . Working example Consider the model in Figure , where {Uğ‘– } ğ‘–= are mutually independent exogenous variables, and {ğ‘“ğ‘– } ğ‘–= are structural linear or  ğ‘‡ be the observed nonlinear equations. Let ğ’™F = ğ‘¥ F , ğ‘¥ F , ğ‘¥ F features belonging to the factual individual, for whom we seek a counterfactual explanation and recommendation. Also, let ğ¼ denote the set of indices corresponding to the subset of endogenous variables that are intervened upon according to the action set A. Then, we obtain a structural counterfactual, ğ’™SCF = FA Fâˆ’ ğ’™F, by applying the Abduction-Action-Prediction steps  as follows: Step . Abduction uniquely determines the value of all exogeğ‘–=, given evidence, {Xğ‘– = ğ‘¥ F nous variables, {ğ‘¢ğ‘– }   Step . Action modifies the SCM according to the hypothetical ğ‘– + ğ›¿ğ‘– , yielding FA: interventions, do{Xğ‘– cid: ğ‘ğ‘– }ğ‘– âˆˆğ¼  where ğ‘ğ‘– = ğ‘¥ ğ¹ X cid:  âˆˆ ğ¼  Â· ğ‘ +  âˆ‰ ğ¼  Â· U, X cid:  âˆˆ ğ¼  Â· ğ‘ +  âˆ‰ ğ¼  Â· U, X cid:  âˆˆ ğ¼  Â· ğ‘ +  âˆ‰ ğ¼  Â· cid:ğ‘“ X, X + Ucid:, X cid:  âˆˆ ğ¼  Â· ğ‘ +  âˆ‰ ğ¼  Â· cid:ğ‘“ X + Ucid:, where Â· denotes the Iverson bracket. Step . Prediction recursively determines the values of all endogenous variables based on the computed exogenous variables {ğ‘¢ğ‘– } ğ‘–= from Step  and FA from Step , as:  cid:  âˆˆ ğ¼  Â· ğ‘ +  âˆ‰ ğ¼  Â· cid:ğ‘¢cid:, ğ‘¥ SCF  cid:  âˆˆ ğ¼  Â· ğ‘ +  âˆ‰ ğ¼  Â· cid:ğ‘¢cid:, ğ‘¥ SCF  cid:  âˆˆ ğ¼  Â· ğ‘ +  âˆ‰ ğ¼  Â· cid:ğ‘“ ğ‘¥ SCF ğ‘¥ SCF  cid:  âˆˆ ğ¼  Â· ğ‘ +  âˆ‰ ğ¼  Â· cid:ğ‘“ ğ‘¥ SCF ğ‘¥ SCF  + ğ‘¢cid:, , ğ‘¥ SCF   + ğ‘¢cid:. = ğ‘– âˆˆ ğ¼  Â· ğ‘¥ F + ğ‘– âˆ‰ ğ¼  Â· cid:ğ‘¥ F  âˆ’ ğ‘“ğ‘– paF ğ‘– cid:. . Demonstration We showcase our proposed formulation by comparing the actions recommended by existing nearest counterfactual explanation methods, as in , to the ones generated by the proposed minimal intervention formulation in . We recall that prior literature has focused on generating counterfactual explanations or CFEbased actions, which as shown above lack optimally or feasibility guarantees in non-independent worlds. Thus, to the best of our knowledge, there exists no baseline approach in the literature that guarantees algorithmic recourse. The experiments below serve as an illustration of the sub-optimality of existing approaches relative to our proposed formulation of recourse via minimal intervention. Section  presents a detailed discussion on practical considerations. We consider two settings: i a synthetic setting where M follows Figure ; and ii a real-world setting based on the german credit dataset , where M follows Figure . We computed the cost of actions as the â„“ norm over normalized feature changes to make effort comparable across features, i.e., costÂ·; ğ’™F = cid:ğ‘– âˆˆğ¼ |ğ›¿ğ‘– |/ğ‘…ğ‘– , where ğ‘…ğ‘– is the range of feature ğ‘–. For the synthetic setting, we generate data following the model in Figure , where we assume X cid: U, X cid: / Â· X + U, with U âˆ¼ $ Â· Poission and U âˆ¼ $ Â· N , ; and the predictive model â„ = sgnX +  Â· X âˆ’ $. Given ğ’™F = $, $ğ‘‡ , solving our formulation, , identifies the optimal action set Aâˆ— = doX cid: ğ‘¥ F  + $ which results in ğ’™*SCF = FAâˆ— Fâˆ’ ğ’™F = $, $ğ‘‡ , whereas solving previous formulations, , yields ğœ¹ âˆ— = $, +$ğ‘‡ resulting in ğ’™*CFE = ğ’™F + ğœ¹ âˆ— = $, $ğ‘‡ . Importantly, while ğ’™*SCF appears to be at a further distance from ğ’™F compared to ğ’™*CFE, achieving the former is less costly than the latter, specifically, costğœ¹ âˆ—; ğ’™F â‰ˆ  costAâˆ—; ğ’™F. Given the setup above, for instance, for the individual ğ’™F = Male, , $, ğ‘‡ identified as a risky customer, solving our formulation, , yields the optimal action set Aâˆ— = do{X cid:  âˆ’ $} which results in ğ’™*SCF = FAâˆ— Fâˆ’ ğ’™F = ğ‘¥ F  + , X cid: ğ‘¥ F Male, , $, ğ‘‡ , whereas solving  yields ğœ¹ âˆ— = N/A, +, , ğ‘‡ resulting in ğ’™*CFE = ğ’™F + ğœ¹ âˆ— = Male, , $, ğ‘‡ . Similar to the toy setting, we observe a % decrease in effort required of the individual when using the action by our method, since our cost function states that waiting for six years to get the credit approved is more costly than applying the following year for a lower âˆ’$ credit amount. We extend our analysis to a population level, and observe that for  negatively affected test individuals, previous approaches suggest actions that are on average % Â± % and % Â± % more costly than our approach when considering, respectively, a logistic regression and a decision tree as the predictive model â„. The demonstrations above confirm our theoretical analysis that MINT-based actions from  are less costly and thus more beneficial for affected individuals than existing CFE-based actions from  that fail to utilize the causal relations between variables.  TOWARDS REALISTIC INTERVENTIONS In Section , we formulated algorithmic recourse by considering the causal relations between features in the real world. Our formulation minimized the cost of actions, which were carried out as structural interventions on the corresponding graph. Each intervention proceeds by unconditionally severing all edges incident on the intervened node, fixing the post-manipulation distribution of a single variable to one deterministic value. While intuitive appealing and powerful, structural interventions are in many ways the simplest type of interventions, and their â€œsimplicity comes at a price: foregoing the possibility of modeling many situations realisiticallyâ€ , . Below, we extend  and  to add flexibility and realism to the types of interventions performed by the individual. Notably, there is nothing inherent to an SCM that a priori determines the form, feasibility, or scope of intervention; instead, these choices are delegated to the individual and are made based on a semantic understanding of the modeled variables. For instance, consider Example , where an individual chooses to increase their bank balance e.g., through borrowing money from family, i.e., a deliberate action/intervention on X while continuing to put aside a portion of their income i.e., retaining the relation X cid: / Â· X + U. Indeed, it would be unwise for a recommendation to suggest abandoning saving habits. In such a scenario, the action would be carried out as an additive a.k.a., soft intervention . Such interventions do not sever graphical edges incident on the intervened node and continue to allow for parents of the node to affect that node. Conversely, in Example , recourse recommendations may suggest performing a structural intervention on temperature, e.g., by creating a climate controlled green-house, to cancel the natural effect of altitude change on temperature. The previous examples illustrate a scenario where an individual/agriculture team actually have the agency to choose which type of intervention to perform. However, it is easy to conceive of examples where such an option does not exist. For instance, as part of a medical systemâ€™s recommendation, we might consider adding  mg/l of insulin to a patient with diabetes with a certain blood insulin level . This action cannot disable pre-existing mechanisms regulating blood insulin levels and therefore, the action can only be performed additively. Conversely, one may also consider another example from the medical domain whereby the only treatment of malignancy may be through a surgical structural amputation. Just as structural interventions were supported in our framework via a closed-form expression see , additive interventions can be encoded through an analogous assignment formulation: ğ‘– cid:. = ğ‘– âˆˆ ğ¼  Â· ğ›¿ğ‘– + cid:ğ‘¥ F ğ‘– + ğ‘“ğ‘– paSCF  âˆ’ ğ‘“ğ‘– paF  The choice of whether interventions should be applied in a additive/soft or structural/hard manner depends on the variable semantic , and should be decided prior to solving . . On the Feasibility of Interventions We saw in Section  that earlier works motivated the addition of feasibility constraints as a means to provide more actionable recommendations for the individual seeking recourse . There, the actionability a.k.a. mutability of a feature was determined based on the feature semantic and value in the factual instance, marking those features which the individual has/lacks the agency to change e.g., bank balance vs. race. While the interchangeable use of definition holds under an independent world, it fails when operating in most real-world settings governed by a set of causal dependencies. We study this subtlety below. Mutable but non-actionable: To encode the conditions for mutable but non-actionable variables, we note that while a variable may not be directly actionable, it may still change as a result of changes to its parents. For example, the financial credit score in Figure  may change as a result of interventions to salary or savings, but is not itself directly intervenable. Therefore, for a non-actionable but mutable variable Xğ‘– , the constraint ğ‘– âˆ‰ ğ¼  =  is sufficient and does not induce any other constraints. ğ‘– ; b the pre-intervention value of other variables ğ‘— } ğ‘— âŠ‚ ğ‘‘ \\ğ‘– ; c the post-intervention value of the intervened ; and d the post-intervention value of other } ğ‘— âŠ‚ ğ‘‘ \\ğ‘– . Such feasibility conditions can easily variable i.e., ğ‘¥ F i.e., {ğ‘¥ F variable i.e., ğ‘¥ SCF variables i.e., {ğ‘¥ SCF be encoded into F ; consider the following scenarios: a an individualâ€™s age can only increase, i.e., ğ‘¥ SCF ğ‘ğ‘”ğ‘’ ; b an individual cannot apply for credit on a temporary visa, i.e., ğ‘¥ F ğ‘£ğ‘–ğ‘ ğ‘ = PERMANENT â‰¥ ğ‘¥ SCF ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘¡ = TRUE; c an individual may undergo heart surgery an additive intervention only if they wonâ€™t remiss due to sustained smoking habits, i.e., ğ‘¥ SCF â„ğ‘’ğ‘ğ‘Ÿğ‘¡ â‰  REMISSION; and â„ğ‘’ğ‘ğ‘Ÿğ‘¡ = SURGERY. d an individual may undergo heart surgery only after their blood pressure is regularized due to medicinal intervention, i.e., ğ‘¥ SCF  DISCUSSION In this paper, we have focused on the problem of algorithmic recourse, i.e., the process by which an individual can change their situation to obtain a desired outcome from a machine learning model. First, using the tools from causal reasoning i.e., structural interventions and counterfactuals, we have shown that in their current form, counterfactual explanations only bring about agency for the individual to achieve recourse in unrealistic settings. In other words, counterfactual explanations do not translate to an optimal or feasible set of actions that would favourably change the prediction of â„ if acted upon. This shortcoming is primarily due to the lack of consideration of causal relations governing the world and thus, the failure to model the downstream effect of actions in the predictions of the machine learning model. In other words, although â€œcounterfactualâ€ is a term from causal language, we observed that existing approaches fall short in terms of taking causal reasoning into account when generating counterfactual explanations and the subsequent recourse actions. Thus, building on the statement by Wachter et al.  that counterfactual explanations â€œdo not rely on knowledge of the causal structure of the world,â€ it is perhaps more appropriate to refer to existing approaches as contrastive, rather than counterfactual, explanations , . In the second work, Mahajan et al.  present a modified version of the distance function in , amending the standard proximity loss between factual and counterfactual instances with a causal regularizer to encourage the counterfactual value of each endogenous variable to be close to the value of that variable had it been assigned via its structural equation. Beyond the uncertainty regarding the strength of regularization which would mean causal relations may not be guaranteed, and why the standard proximity loss only iterates over the exogenous variables which from a causal perspective, are characteristics that are shared across counterfactual worlds , and evaluating various desirable model properties, such as robustness ,  or fairness , , , . Besides this, it has been shown that designers of interpretable machine learning systems use counterfactual explanations for predicting model behavior  or uncovering inaccuracies in the data profile of individuals . Complementing these offerings of counterfactual explanations, we offer minimal interventions as a way to guarantee algorithmic recourse in general settings, which is not implied by counterfactual explanations. Future work. In future work, we aim to focus on overcoming the main assumption of our formulation: the availability of the true world model, M. An immediate first step involves learning the true world model partially or fully , , , and studying potential inefficiencies that may arise from partial or imperfect knowledge of the causal model governing the world. Furthermore, while additive noise models are a broadly used class of SCMs for modeling real-world systems, further investigation into the effects of confounders non-independent noise variables, the presence of only the causal graph, as well as cyclic graphical models for time series data e.g., conditional interventions, would extend the reach of algorithmic recourse to even broader settings. ACKNOWLEDGMENTS The authors would like to thank AdriÃ¡n Javaloy BornÃ¡s and Julius von KÃ¼gelgen for their valuable feedback on drafts of the manuscript. footnote , this approach suffers from a primary limitation in its causal treatment: the causal regularizer would penalize any variable whose value deviated away from its structurally assigned value. While on the surface this â€œpreservation of causal relationsâ€ seems beneficial, such an approach would discourage interventions additive or structural on non-root variables, which would, by design, change the value of the intervened-upon variable away from its structurally assigned value. Instead, the regularizer would encourage interventions on variables that would not be penalized as such, i.e., root variables, which may not be contextually acceptable as root notes typically capture sensitive characteristic of the individual e.g., birthplace, age, gender. The authors suggest in the Appendix of  that one may consider those variables, upon which structural interventions are to be performed, as exogenous. In this manner, interventions would not be penalized and down-stream effects of interventions would still be preserved when searching for the nearest counterfactual instance. We argue, however, that such an approach suffers from the same limitations as other CFE-based recourse approaches presented in Section . in that a returned counterfactual instance would not imply feasible or optimal actions for recourse. Finally, without an explicit abduction step and without assumptions on the form of structural equations, it is unclear how the authors infer and combine individual-specific characteristics as embedded in the background variables with the effect of ancestral changes to compute the counterfactual. We believe the problems above will be mostly resolved when minimizing over the cost of actions instead of distance over counterfactuals as we have done in this work. â‰  ğ‘¥ F ğ‘™ exists a descendant of ğ‘‹ğ‘˜ for which the value of its ancestors change under intervention, i.e., âˆƒ ğ‘™ âˆˆ dXğ¼  s.t. ğ‘“ğ‘™ paSCF ğ‘™  â‰  . and thus ğ’™SCF â‰  ğ’™*CFE := ğ’™F + ğœ¹ âˆ—. Our proof igThus, ğ‘¥ SCF nores special cases such as piece-wise constant structural equations, where for some ğ›¿ âˆ— ğ‘– â‰  , the descendant of ğ‘‹ğ‘– remains invariant. These rare cases can be thought of as locally violating causal miniâ–¡ mality , Sec. . and are thus disregarded.  âˆ’ ğ‘“ğ‘™ paF A. Proof of Corollary . If the true world M is independent, i.e, all the obCorollary .. served features are root-nodes, then CFE-based actions always guarantee recourse. Proof. If the true world M is independent, then by definition the set of descendants for all variables is the empty set. Thus, the â–¡ statement follows directly from Proposition .. A PROOFS A. Proof of Proposition . Proposition .. A CFE-based action, ACFE, where ğ¼ = {ğ‘– | ğ›¿ âˆ— ğ‘– â‰  }, performed by individual ğ’™F, in general results in the structural counterfactual, ğ’™SCF = ğ’™*CFE := ğ’™F + ğœ¹ âˆ—, and thus guarantees recourse i.e., â„ğ’™SCF â‰  â„ğ’™F, if and only if, the set of descendants of the acted upon variables, determined by ğ¼ , is the empty set. Proof. The setting assumes that the causal graph G is available such that the parent set for each variable is known. Let dğ‘‹  and ndğ‘‹  denote the sets of descendants and non-descendants of the variable ğ‘‹ according to G, respectively. For multiple intervenedupon variables, we define: Xğ¼ := {ğ‘‹ğ‘– }ğ‘– âˆˆğ¼ , ndXğ¼  := âˆ©ğ‘– âˆˆğ¼ ndğ‘‹ğ‘– , dXğ¼  := X \\ Xğ¼ âˆª ndXğ¼ . Note that, by definition, Xğ¼ , ndXğ¼ , and dXğ¼  form a partition of the set of all variables X. For ease of exposition, we define ğ’™SCF = ğ’™*CFE := ğ’™F + ğœ¹ âˆ— cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid: cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid:cid: cid:cid: cid: cid: p â‡â‡’ dXğ¼  = âˆ… cid:cid:cid:cid:cid:cid: cid: cid: cid:cid:cid:cid:cid:cid: cid:cid: q  âˆ’ ğ‘“ğ‘– paF ğ‘–  ï£±ï£´ï£´ï£´ï£² ï£´ï£´ï£´ ï£³  âˆ’ ğ‘“ğ‘– paF ğ‘–   âˆ’ ğ‘“ğ‘– paF ğ‘–  ğ‘– âˆˆ ğ¼ ğ‘– âˆˆ dXğ¼  ğ‘– âˆˆ ndXğ¼  variables unaffected, we have that Consequently, ğ‘“ğ‘– paSCF ğ‘– In summary, we have = A. which, upon realising that ğ›¿ âˆ— ğ’™SCF = ğ’™*CFE := ğ’™F + ğœ¹ âˆ— as desired. ğ‘– â‰   =â‡’ ğ‘– âˆˆ ğ¼ , reduces to \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}